

                             Classification Algo
                                /           \
                               /             \
                              /               \
                Support Vector Machine    Naive Bayes
                
Bayes Probablity Theorm and Naive Principle based.


Introduce medium.com ( Hiring Requirenment )

LinkedIn Account Creation

SVM can be used for Regression Problems also - Support Vector Regressor ( SVR )

Define NLP ( review of movies, restaurant , emotions, sentiment )

SVM is popular for solving NLP ( Natural Language Processsing )

Sentiment Analysis in NLP is used to get the polarity, opinion mining, positive / Negative

NLP is text processing to detect text classification 


Either Naive Bayes / SVM is used

Two types of dataset  
1. Labelled (feature,lalels)  2. Unlabelled (feature)

Photograph of the class with the name and without names is an example to explain

Prior Concept to understand SVM

1. Linearly seperable data
2. Linearly Inseperable data 

Show image svm-01.png

Straight decission bounary seperates the data 

Show image svm-03.png
DB just like Logistic Regression 
Hyperplane in 2D as a line

Supprt Vector is the nearest point to the line ( perpendicular distance is least)

Show image svm-02.png

Best hyperplane to seperate, whose margin is maximum, if data is exactly seperable

For linearly non seperable data

Show image svm-03.png part 2

How to classify such data 

To Solve this we can use SVM, best use if for making linearly non seperable data

How to create hyperplane ?

Assume it as a rubber sheet, the points which are plotted on 2D plane

Elastic pull the points in the centre.

Green points would be in the 2D Plane, Blue points 

Now data is coming in 3D, z axis

Now cut the rubber with a A4 size paper(hyperplane)

We need to convert into higher dimension

How to convert 2D to 3D or Low Dimension to Higher Dimension, kernel function does the same work

SVM with polynomial kernel visualisation in YouTube to explain with animation 

Gaussian method we wil be using as a default kernel function, to convert 2D to 3D

Cut will create a rinf in 3D

But the data is in 2D

Now project the ring with a torch on the 2D plane

Put torch to shadow on 2D

This is the way SVM works 

SVM Kernel Function 
    1 Polynomial
    2 Gaussian 
    3 RBF
    4 Hyperbolic
    5 Sigmoid


demo_kernel_SVM.py

Its a classification problem

Go line by line

SVM ( 1. SVC for classification and 2. SVR for Regressoin )

Try with rbf, linear, poly

Poly takes alot of time to create the visualisation 

Naive Bayes PDF

demo_naive_bayes.py

Naive Bayes 
    1. Gaussian  ( for continuous data like numerical )
    2. Bernolli  ( for discrete values like 0 or 1 )
    3. Multinomial ( for text data )
    
 

















 
  


"""
https://www.youtube.com/watch?v=3liCbRZPrZA&feature=youtu.be
https://www.youtube.com/watch?v=1NxnPkZM9bc

https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/
https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/
https://data-flair.training/blogs/svm-kernel-functions/
"""

https://www.machinelearningplus.com/predictive-modeling/how-naive-bayes-algorithm-works-with-example-and-full-code/

https://en.wikipedia.org/wiki/Additive_smoothing

"""
Bernoulli Naive Bayes : It assumes that all our features are binary such that they take only two values. Means 0s can represent “word does not occur in the document” and 1s as "word occurs in the document". Email classifier .

Multinomial Naive Bayes : Its is used when we have discrete data (e.g. movie ratings ranging 1 and 5 as each rating will have certain frequency to represent). In text learning we have the count of each word to predict the class or label.

Gaussian Naive Bayes : Because of the assumption of the normal distribution, Gaussian Naive Bayes is used in cases when all our features are continuous. For example in Iris dataset features are sepal width, petal width, sepal length, petal length. So its features can have different values in data set as width and length can vary. We can’t represent features in terms of their occurrences. This means data is continuous. Hence we use Gaussian Naive Bayes here.

"""

"""
http://kenzotakahashi.github.io/naive-bayes-from-scratch-in-python.html
https://nlp.stanford.edu/IR-book/
https://blog.sicara.com/naive-bayes-classifier-sklearn-python-example-tips-42d100429e44
https://www.datacamp.com/community/tutorials/naive-bayes-scikit-learn
Naive Bayes Explainer in the email(forsklabs)

"""

#Iris dataset
https://dataaspirant.com/2017/01/25/svm-classifier-implemenation-python-scikit-learn/

https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/


https://adrianromano.com/dealing-with-classification-problems-in-data-science-classification-algorithms-evaluation-metrics/